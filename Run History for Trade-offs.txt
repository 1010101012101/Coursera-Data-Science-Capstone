Run History for Trade-offs

"","filesToLoad","NoLinesEachFileOrFraction","LocToReadLines","cumPercent","trainPercent","trainSkipPenalty","predictSkipPenalty","noWordsToPredict","removeStopWords","removeWordSuffixes","timeToTrain","sizeOfTrainDB","NoTrainingFileLines","noPredictorWordsTotal","noPredictedWordsTotal","Accuracy","avgTimeToPredict","sdTimeToPredict"

Using all files vs. individual files for training/predicting
"1",7,0.005,"top",1,0.6,2,2,10,FALSE,FALSE,8492,6586512,7650,38674,28956,0.3212,0.036,0.022
"2",1,0.005,"top",1,0.6,2,2,10,FALSE,FALSE,1792,3527088,7650,21664,17372,0.3235,0.022,0.013
"3",2,0.005,"top",1,0.6,2,2,10,FALSE,FALSE,1257,3402608,7650,20814,17469,0.3726,0.02,0.015
"4",4,0.005,"top",1,0.6,2,2,10,FALSE,FALSE, 302,1365112,7650, 8556, 7858,0.3726,0.012,0.012
	CONCLUSION: Training using Twitter only or News only was better than blogs only or all together
				News only DB smallest while giving highest results


Trade-off of removing stop words and removing word suffixes
"1",7,0.005,"top",1,0.6,2,2,10,FALSE,FALSE,8492,6586512,7650,38674,28956,0.3212,0.036,0.022
"5",7,0.005,"top",1,0.6,2,2,10,TRUE,FALSE,7740,6586512,7650,38674,28956,0.2909,0.035,0.018
"6",7,0.005,"top",1,0.6,2,2,10,FALSE,TRUE,7597,6604040,7650,38664,29216,0.1878,0.04,0.023
"7",7,0.005,"top",1,0.6,2,2,10,TRUE,TRUE,7619,6604040,7650,38664,29216,0.1954,0.039,0.021
	CONCLUSION: Not removing suffixes was much better than removing suffixes
				Keeping suffixes, not removing stopwords for was marginally better than removing them


Trade-off of keeping various percentages of bigrams (all tri+grams are kept)
"11",7,0.005,"top",0.25,0.6,2,2,10,FALSE,FALSE,5467,2148008,7650,15804, 8854,0.166 ,0.015,0.011
 "8",7,0.005,"top",0.5 ,0.6,2,2,10,FALSE,FALSE,6026,3800176,7650,23771,17776,0.2154,0.022,0.016
 "9",7,0.005,"top",0.9 ,0.6,2,2,10,FALSE,FALSE,9019,6076712,7650,35979,27145,0.3024,0.034,0.019
"10",7,0.005,"top",0.99,0.6,2,2,10,FALSE,FALSE,9264,6535384,7650,38387,28785,0.319 ,0.036,0.02
 "1",7,0.005,"top",1   ,0.6,2,2,10,FALSE,FALSE,8492,6586512,7650,38674,28956,0.3212,0.036,0.022
 	CONCLUSION: Keeping more results in higher accuracy. The relationship is better than linear


AFTER UPDATING LEARNING TO INCLUDE UP TO  4-Grams and skip-1's

Trade-off for % of training nGrams to keep
"1",1,100,"top",1   ,0.6,2,2,10,FALSE,FALSE,129,1062056,60,11026,1562,0.9211,0.011,0.01  - 100% trainDB, 92% Accuracy
"2",1,100,"top",0.9 ,0.6,2,2,10,FALSE,FALSE,125, 950216,60,10102,1360,0.8421,0.015,0.01  -  90% trainDB, 84% Accuracy
"3",1,100,"top",0.75,0.6,2,2,10,FALSE,FALSE,106, 730088,60, 8251, 813,0.5   ,0.012,0.009 -  75% trainDB, 50% Accuracy
"4",1,100,"top",0.5 ,0.6,2,2,10,FALSE,FALSE, 89, 396304,60, 4451, 609,0.0263,0.013,0.011 -  50% trainDB,  3% Accuracy
	CONCLUSIONS:	Very good accuracy (with 10 guesses) after 4-Gram, skip=1 update
					Big drop-off with drop in cumulative percentage kept. May want to do 0.99, 0.95 to refine


Trade-off for number of guesses allowed
"1",1,100,"top",1,0.6,2,2,10,FALSE,FALSE,129,1062056,60,11026,1562,0.9211,0.011,0.01  - 10 guesses, 92% Accuracy
"5",1,100,"top",1,0.6,2,2, 7,FALSE,FALSE,129,1062056,60,11026,1562,0.9211,0.011,0.011 -  7 guesses, 92% Accuracy
"6",1,100,"top",1,0.6,2,2, 5,FALSE,FALSE,129,1062056,60,11026,1562,0.8947,0.013,0.014 -  5 guesses, 89% Accuracy
"7",1,100,"top",1,0.6,2,2, 2,FALSE,FALSE,129,1062056,60,11026,1562,0.8684,0.012,0.011 -  2 guesses, 87% Accuracy
"8",1,100,"top",1,0.6,2,2, 1,FALSE,FALSE,129,1062056,60,11026,1562,0.6316,0.011,0.008 -  1 guess,   63% Accuracy
	CONCLUSION:		Relatively slow drop-off with number of guesses - higher ranked guesses are getting right answer


Evaluation of accuracy with increasing number of training, testing lines
 "1",1, 100,"top",1   ,0.6,2,2,10,FALSE,FALSE, 129,1062056, 60,11026,1562,0.9211,0.011,0.01  -  100 lines, 92% Accuracy
 "9",1, 500,"top",1   ,0.6,2,2,10,FALSE,FALSE,1943,4202248,300,43450,4685,0.8223,0.021,0.014 -  500 lines, 82% Accuracy
"10",1,1000,"top",1   ,0.6,2,2,10,FALSE,FALSE,6501,7514896,600,77259,7256,0.8051,0.031,0.025 - 1000 lines, 81% Accuracy
	CONCLUSION:		Accuracy drops-off with increasing training, testing lines but based on a small sample does appear to be leveling-out

Evaluation of accuracy with increasing number of training, testing lines - with up to 5-grams, skip-2 predictions
"1",1, 100,"top",1,0.6,2,2,10,FALSE,FALSE,NA,NA,NA,NA,NA,0.9211,0.05 ,0.027 -  100 lines, 92% Accuracy
"2",1, 500,"top",1,0.6,2,2,10,FALSE,FALSE,NA,NA,NA,NA,NA,0.8274,0.134,0.074 -  500 lines, 83% Accuracy
"3",1,1000,"top",1,0.6,2,2,10,FALSE,FALSE,NA,NA,NA,NA,NA,0.8152,0.22 ,0.126 - 1000 lines, 82% Accuracy
	CONCLUSION:		Similar results to before prediction update

Trade-off for number of guesses allowed - with updated prediction algorithm
 "1",1,100,"top",1,0.6,2,2, 1,FALSE,FALSE,NA,NA,NA,NA,NA,0.6579,0.05, 0.028
 "2",1,100,"top",1,0.6,2,2, 2,FALSE,FALSE,NA,NA,NA,NA,NA,0.6579,0.052,0.031
 "3",1,100,"top",1,0.6,2,2, 3,FALSE,FALSE,NA,NA,NA,NA,NA,0.7105,0.054,0.029
 "4",1,100,"top",1,0.6,2,2, 4,FALSE,FALSE,NA,NA,NA,NA,NA,0.7632,0.054,0.031
 "5",1,100,"top",1,0.6,2,2, 5,FALSE,FALSE,NA,NA,NA,NA,NA,0.8947,0.013,0.014
 "6",1,100,"top",1,0.6,2,2, 6,FALSE,FALSE,NA,NA,NA,NA,NA,0.8947,0.052,0.027
 "7",1,100,"top",1,0.6,2,2, 7,FALSE,FALSE,NA,NA,NA,NA,NA,0.9211,0.011,0.011
 "8",1,100,"top",1,0.6,2,2, 8,FALSE,FALSE,NA,NA,NA,NA,NA,0.9211,0.056,0.031
 "9",1,100,"top",1,0.6,2,2, 9,FALSE,FALSE,NA,NA,NA,NA,NA,0.9211,0.053,0.029
"10",1,100,"top",1,0.6,2,2,10,FALSE,FALSE,NA,NA,NA,NA,NA,0.9211,0.05 ,0.027
