

Report Notes
  - Questions to be solved
  	- how to increase accuracy
  	- how to use more of the words database
  	- should I use only twitter data to predict twitter, etc.
  	- How many words allowed to predict
  	- I'm only predicting based on one word, any of the last four with more weight on the word closer to the word trying to be predicted.
  		- Do I need to add prediction based on multi-word sequences? If so, remove based on single n-2, n-3, n-4 word?
  	- No smoothing done based on using Sparse Matrix. Smoothing would mean using non-zero values
  		- or use zero values as 1 equivalents - i.e. take top x even if zero but how to judge many, many zeros?

  - Trade-off graphs to show or plan to do
  	- Accuracy / Train Time / Predict Time / DB Size vs.
  		- Number of lines, DB Size, Train/Predict Skip Penalties, # of predict words allowed
  		- trade-off how deep - n-1, n-1,2, n-1,2,3, n-1,2,3,4, ...




https://en.wikipedia.org/wiki/Katz%27s_back-off_model
https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf



BUILDING
	For each n-Gram, Skip-m - Add m-n to count for each one found. Then drop lowest % by count sums for predictor and predicted   ***********************************
	Eliminate trainSkipPenalty, testSkipPenalty

Contained In:           6G   5G   4G   3G   2G
	 1. 5-Grams         (     n-4, n-3, n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=5
	 1. 5-Grams, Skip-1 (n-5,      n-3, n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4
	 1. 5-Grams, Skip-1 (n-5, n-4,      n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4
	 1. 5-Grams, Skip-1 (n-5, n-4, n-3,      n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4
	 1. 5-Grams, Skip-1 (n-5, n-4, n-3, n-2,      n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4

	 2. 4-Grams         (          n-3, n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4
	 2. 4-Grams, Skip-1 (     n-4,      n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=3
	 2. 4-Grams, Skip-1 (     n-4, n-3,      n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=3
	 2. 4-Grams, Skip-1 (     n-4, n-3, n-2,      n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=3

	 3. 3-Grams         (               n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=3
	 3. 3-Grams, Skip-1 (          n-3,      n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=2
	 3. 3-Grams, Skip-1 (          n-3, n-2,      n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=2
	 4. 2-Grams         (                    n-1, n) - with    stopword removal. Suffix removal depending on switch - to be tested - Count=2
	 4. 2-Grams, Skip-1 (               n-2,      n) - with    stopword removal. Suffix removal depending on switch - to be tested - Count=1


PREDICTING
	for each n-Gram, Skip-m - Use m-n-x as multiplier to count form MarkovChain Matrix. Then collect as many as possible solutions in timely manner and rank by points
		x is permutation penalty, probably 1 is sufficient
In Priority Order, possibly eliminate cut-offs along the way if fast enough to do all
	- Start by doing Corpus with and without stopword removal - do at the same time?

	PRIORITY 1
		5. 5-grams         (          n-4, n-3, n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=5

	  51A. 5-grams, Skip-1 (     n-5,      n-3, n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=4
	  51B. 5-grams, Skip-1 (     n-5, n-4,      n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=4
	  51C. 5-grams, Skip-1 (     n-5,      n-3,      n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=4
	  51D. 5-grams, Skip-1 (     n-5,      n-3, n-2,      n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=4

	  52A. 5-grams, Skip-2 (n-6,           n-3, n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52B. 5-grams, Skip-2 (n-6,      n-4,      n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52C. 5-grams, Skip-2 (n-6,      n-4, n-3,      n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52D. 5-grams, Skip-2 (n-6,      n-4, n-3, n-2,      n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52E. 5-grams, Skip-2 (n-6, n-5,           n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52F. 5-grams, Skip-2 (n-6, n-5,      n-3,      n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52G. 5-grams, Skip-2 (n-6, n-5,      n-3, n-2,      n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52H. 5-grams, Skip-2 (n-6, n-5, n-4,           n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52I. 5-grams, Skip-2 (n-6, n-5, n-4,      n-2,      n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52J. 5-grams, Skip-2 (n-6, n-5, n-4, n-3,           n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3


		4. 4-grams         (               n-3, n-2, n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=4

	  41A. 4-grams, Skip-1 (          n-4,      n-2, n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=3
	  41B. 4-grams, Skip-1 (          n-4, n-3,      n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=3
	  41C. 4-grams, Skip-1 (          n-4, n-3, n-2,      n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=3

	  42A. 4-grams, Skip-2 (     n-5,           n-2, n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42B. 4-grams, Skip-2 (     n-5,      n-3,      n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42C. 4-grams, Skip-2 (     n-5,      n-3, n-2,      n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42D. 4-grams, Skip-2 (     n-5, n-4,           n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42E. 4-grams, Skip-2 (     n-5, n-4,      n-2,      n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42F. 4-grams, Skip-2 (     n-5, n-4, n-3,           n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2


	 	3. 3-grams         (                    n-2, n-1, n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=3

	  31A. 3-grams, Skip-1 (               n-3,      n-1, n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=2
	  31B. 3-grams, Skip-1 (               n-3, n-2,      n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=2

	  32A. 3-grams, Skip-2 (          n-4,           n-1, n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=1
	  32B. 3-grams, Skip-2 (          n-4,      n-2,      n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=1
	  32C. 3-grams, Skip-2 (          n-4, n-3,           n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=1


		2. 2-grams         (                         n-1, n) - P(word) = P(2-gram with word)/P(1-gram) x Multiplier=2
	   21. 2-grams, Skip-1 (                    n-2,      n) - P(word) = P(2-gram with word)/P(1-gram) x Multiplier=1
		Possibly take highest probability(ies) from above.

	PRIORITY 2
		If no options seen above, start permutations of n-grams - same as above but decrease multipliers by 1 - for later

*************   Save NUMBER from above in TestList to document which method found given words **************

Hints, tips, and tricks

As you develop your prediction model, two key aspects that you will have to keep in mind are the size and runtime of the algorithm. These are defined as:

	- Size: the amount of memory (physical RAM) required to run the model in R
	- Runtime: The amount of time the algorithm takes to make a prediction given the 
	  a cceptable input

	- Your goal for this prediction model is to minimize both the size and runtime of the
	  model in order to provide a reasonable experience to the user.

Ultimately, your model will need to run in a Shiny app that runs on the shinyapps.io server.




So far you have used basic models to understand and predict words. In this next task, your goal is to use all the resources you have available to you (from the Data Science Specialization, resources on the web, or your own creativity) to improve the predictive accuracy while reducing computational runtime and model complexity (if you can). Be sure to hold out a test set to evaluate the new, more creative models you are building.

Tasks to accomplish


*** For nGrams, calculate power = freq of ngram / freq or predictor calculate cumulative and eliminate above a certain percentage (cumPercent) or with 'freq of ngram' below a certain number (2) - this means having to add 'freq of ngram' to solutions list temporarily


PRESENTATION
	Slide 1
		- Title
		- executive summary
	Slide 2
		- History
		- motivation
	Slide 3
		- Prediction Development
		- Trade-off's Evaluated
	Slide 4
		- Accuracy summary
		- Trade-off graphs 
		- graph of number of lines used, time to train, accuracy as basis for trade-off
	Slide 5
		- App Pictures
		- explanation of how it works



When looking at words that aren't predicted, see if they're in dropped words
	Do these words need to be predictable but not used for predictions?


"3",2, 500,"top",1   ,0.6,2,2,10,FALSE,FALSE
"4",4, 500,"top",1   ,0.6,2,2,10,FALSE,FALSE
"5",7, 500,"top",1   ,0.6,2,2,10,FALSE,FALSE
"7",1,1000,"top",1   ,0.6,2,2,10,FALSE,FALSE
