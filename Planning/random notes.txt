

Report Notes
  - Questions to be solved
  	- how to increase accuracy
  	- how to use more of the words database
  	- should I use only twitter data to predict twitter, etc.
  	- How many words allowed to predict
  	- I'm only predicting based on one word, any of the last four with more weight on the word closer to the word trying to be predicted.
  		- Do I need to add prediction based on multi-word sequences? If so, remove based on single n-2, n-3, n-4 word?
  	- No smoothing done based on using Sparse Matrix. Smoothing would mean using non-zero values
  		- or use zero values as 1 equivalents - i.e. take top x even if zero but how to judge many, many zeros?

  - Trade-off graphs to show or plan to do
  	- Accuracy / Train Time / Predict Time / DB Size vs.
  		- Number of lines, DB Size, Train/Predict Skip Penalties, # of predict words allowed
  		- trade-off how deep - n-1, n-1,2, n-1,2,3, n-1,2,3,4, ...




https://en.wikipedia.org/wiki/Katz%27s_back-off_model
https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf



BUILDING
	For each n-Gram, Skip-m - Add m-n to count for each one found. Then drop lowest % by count sums for predictor and predicted   ***********************************
	Eliminate trainSkipPenalty, testSkipPenalty

Contained In:           6G   5G   4G   3G   2G
	 1. 5-Grams         (     n-4, n-3, n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=5
	 1. 5-Grams, Skip-1 (n-5,      n-3, n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4
	 1. 5-Grams, Skip-1 (n-5, n-4,      n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4
	 1. 5-Grams, Skip-1 (n-5, n-4, n-3,      n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4
	 1. 5-Grams, Skip-1 (n-5, n-4, n-3, n-2,      n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4

	 2. 4-Grams         (          n-3, n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=4
	 2. 4-Grams, Skip-1 (     n-4,      n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=3
	 2. 4-Grams, Skip-1 (     n-4, n-3,      n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=3
	 2. 4-Grams, Skip-1 (     n-4, n-3, n-2,      n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=3

	 3. 3-Grams         (               n-2, n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=3
	 3. 3-Grams, Skip-1 (          n-3,      n-1, n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=2
	 3. 3-Grams, Skip-1 (          n-3, n-2,      n) - without stopword removal. Suffix removal depending on switch - to be tested - Count=2
	 4. 2-Grams         (                    n-1, n) - with    stopword removal. Suffix removal depending on switch - to be tested - Count=2
	 4. 2-Grams, Skip-1 (               n-2,      n) - with    stopword removal. Suffix removal depending on switch - to be tested - Count=1


PREDICTING
	for each n-Gram, Skip-m - Use m-n-x as multiplier to count form MarkovChain Matrix. Then collect as many as possible solutions in timely manner and rank by points
		x is permutation penalty, probably 1 is sufficient
In Priority Order, possibly eliminate cut-offs along the way if fast enough to do all
	- Start by doing Corpus with and without stopword removal - do at the same time?

	PRIORITY 1
		5. 5-grams         (          n-4, n-3, n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=5

	  51A. 5-grams, Skip-1 (     n-5,      n-3, n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=4
	  51B. 5-grams, Skip-1 (     n-5, n-4,      n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=4
	  51C. 5-grams, Skip-1 (     n-5,      n-3,      n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=4
	  51D. 5-grams, Skip-1 (     n-5,      n-3, n-2,      n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=4

	  52A. 5-grams, Skip-2 (n-6,           n-3, n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52B. 5-grams, Skip-2 (n-6,      n-4,      n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52C. 5-grams, Skip-2 (n-6,      n-4, n-3,      n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52D. 5-grams, Skip-2 (n-6,      n-4, n-3, n-2,      n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52E. 5-grams, Skip-2 (n-6, n-5,           n-2, n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52F. 5-grams, Skip-2 (n-6, n-5,      n-3,      n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52G. 5-grams, Skip-2 (n-6, n-5,      n-3, n-2,      n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52H. 5-grams, Skip-2 (n-6, n-5, n-4,           n-1, n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52I. 5-grams, Skip-2 (n-6, n-5, n-4,      n-2,      n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3
	  52J. 5-grams, Skip-2 (n-6, n-5, n-4, n-3,           n) - P(word) = P(5-gram, skip-1 with word)/P(4-gram, skip-1) x Multiplier=3


		4. 4-grams         (               n-3, n-2, n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=4

	  41A. 4-grams, Skip-1 (          n-4,      n-2, n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=3
	  41B. 4-grams, Skip-1 (          n-4, n-3,      n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=3
	  41C. 4-grams, Skip-1 (          n-4, n-3, n-2,      n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=3

	  42A. 4-grams, Skip-2 (     n-5,           n-2, n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42B. 4-grams, Skip-2 (     n-5,      n-3,      n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42C. 4-grams, Skip-2 (     n-5,      n-3, n-2,      n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42D. 4-grams, Skip-2 (     n-5, n-4,           n-1, n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42E. 4-grams, Skip-2 (     n-5, n-4,      n-2,      n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2
	  42F. 4-grams, Skip-2 (     n-5, n-4, n-3,           n) - P(word) = P(4-gram with word)/P(3-gram) x Multiplier=2


	 	3. 3-grams         (                    n-2, n-1, n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=3

	  31A. 3-grams, Skip-1 (               n-3,      n-1, n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=2
	  31B. 3-grams, Skip-1 (               n-3, n-2,      n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=2

	  32A. 3-grams, Skip-2 (          n-4,           n-1, n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=1
	  32B. 3-grams, Skip-2 (          n-4,      n-2,      n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=1
	  32C. 3-grams, Skip-2 (          n-4, n-3,           n) - P(word) = P(3-gram with word)/P(2-gram) x Multiplier=1


		2. 2-grams         (                         n-1, n) - P(word) = P(2-gram with word)/P(1-gram) x Multiplier=2
	   21. 2-grams, Skip-1 (                    n-2,      n) - P(word) = P(2-gram with word)/P(1-gram) x Multiplier=1
		Possibly take highest probability(ies) from above.

	PRIORITY 2
		If no options seen above, start permutations of n-grams - same as above but decrease multipliers by 1 - for later

*************   Save NUMBER from above in TestList to document which method found given words **************

Hints, tips, and tricks

As you develop your prediction model, two key aspects that you will have to keep in mind are the size and runtime of the algorithm. These are defined as:

	- Size: the amount of memory (physical RAM) required to run the model in R
	- Runtime: The amount of time the algorithm takes to make a prediction given the 
	  a cceptable input

	- Your goal for this prediction model is to minimize both the size and runtime of the
	  model in order to provide a reasonable experience to the user.

Ultimately, your model will need to run in a Shiny app that runs on the shinyapps.io server.




So far you have used basic models to understand and predict words. In this next task, your goal is to use all the resources you have available to you (from the Data Science Specialization, resources on the web, or your own creativity) to improve the predictive accuracy while reducing computational runtime and model complexity (if you can). Be sure to hold out a test set to evaluate the new, more creative models you are building.

Tasks to accomplish


*** For nGrams, calculate power = freq of ngram / freq or predictor calculate cumulative and eliminate above a certain percentage (cumPercent) or with 'freq of ngram' below a certain number (2) - this means having to add 'freq of ngram' to solutions list temporarily

PRESENTATION GOALS
	The goal of this exercise is to "pitch" your data product to your boss or an investor. The slide deck is constrained to be 5 slides or less and should: (1) explain how your model works, (2) describe its predictive performance quantitatively and (3) show off the app and how it works.

	Tasks to accomplish

	- Create a slide deck promoting your product. Write 5 slides using RStudio Presenter 
	  explaining your product and why it is awesome!

Questions to consider
	- How can you briefly explain how your predictive model works?
	- How can you succinctly quantitatively summarize the performance of your prediction
	  algorithm?
	- How can you show the user how the product works?

Tips, tricks, and hints

The Rstudio presentation information is available here (https://support.rstudio.com/hc/en-us/articles/200486468-Authoring-R-Presentations).

PRESENTATION
	Slide 1
		- Title - Next Word Prediction Demonstraton Algorithm
		- executive summary
			Next word prediction is used in many forms today from web browser/search
			engine text entry to smart phone text entry to enable fast typing with and
			to suggest items the user may not have considered. This project is an exercise
			in creating a 'next word' prediction algorithm and shiny app to demonstrate the
			algorithm working.
			This demonstration uses a learned database based on the 'en_US.news.txt' dataset
			found in the Coursera-SwiftKey database (https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). A random 1% of the 200 MB file was used
			with 60% of that list used for training and the remaining 40% used for algorithm
			validation.
			The result, as demonstrated in the shiny application, is an accuracy rate of
			58% to 79% depending on number of guesses allowed.

The development of this project has four main sections and status at the time
of this interim report:  

   1. Exploratory Analysis of the supplied text files  
        - status: Complete but may be expanded in future  

   2. Development of databases to be used to predict 'next words'  
        - status: Basic algorithm in-place. Some iterations evaluated. Much
                  improvement needed to increase accuracy.  

   3. Evaluation of 'next word' prediction and optimization of many parameters
      including database size, prediction speed and othes  
        - status: Evaluation method in-place. Optimization in-process.
   4. Implementing best prediction method in shiny web application for
      demonstration.  
        - status: Not started. Basic application outline planned.  
  
	Slide 2
		- History
		- motivation
	Slide 2
		- Prediction Development
		- Trade-off's Evaluated
	Slide 3
		- Accuracy summary
		- Trade-off graphs 
		- graph of number of lines used, time to train, accuracy as basis for trade-off
	Slide 4
		- More trade-off graphs
	Slide 5
		- App Pictures
		- explanation of how it works



When looking at words that aren't predicted, see if they're in dropped words
	Do these words need to be predictable but not used for predictions?


"3",2, 500,"top",1   ,0.6,2,2,10,FALSE,FALSE
"4",4, 500,"top",1   ,0.6,2,2,10,FALSE,FALSE
"5",7, 500,"top",1   ,0.6,2,2,10,FALSE,FALSE
"7",1,1000,"top",1   ,0.6,2,2,10,FALSE,FALSE




