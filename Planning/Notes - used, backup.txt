How can you efficiently store an n-gram model (think Markov Chains)?
  - sparse matrix - OK?
  - only use n-1 state for prediction
  - If n-1 state not in history, can use n-2, n-3 as surrogate for n-1?
  - probability in Markov Chain based on frequency in training database

          x <- c(1,2,1,1,3,4,4,1,2,4,1,4,3,4,4,4,3,1,3,2,3,3,3,4,2,2,3)
          p <- matrix(nrow = 4, ncol = 4, 0)
          for (t in 1:(length(x) - 1)) p[x[t], x[t + 1]] <- p[x[t], x[t + 1]] + 1
          for (i in 1:4) p[i, ] <- p[i, ] / sum(p[i, ])

      or

          x <- c(1,2,1,1,3,4,4,1,2,4,1,4,3,4,4,4,3,1,3,2,3,3,3,4,2,2,3)
          xChar<-as.character(x)
          library(markovchain)
          mcX<-markovchainFit(xChar)$estimate
          mcX
          as(mcX,"data.frame")
  - 


for training, try
  - full list of pairs, trips, quads
  - top 50% of each
  - top 90% of each
  - top 99% of each
  - eliminate all with freq=1


Prediction Methods & Priorities
  1. last 3 words in sequence
  2. last 2 words in sequence
  3. last 1 word in sequence
  4. 3 of last 4 words in any order - maybe this is finding bad solutions with higher probabilities overiding 1-3 above, evaluate eliminating
  5. 2 of last 3 words in any order - maybe this is finding bad solutions with higher probabilities overiding 1-3 above, evaluate eliminating
  6. 2nd to last word in 1 word - consider expanding to 3rd and 4th to last word if 1-3, 6 don't find a 'probable' solution
  7. Pick top three and judge ok if in that list




  